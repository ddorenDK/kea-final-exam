import requests
import camelot
import os
import json
import shutil

#TODO
#Move this all to a better location
epd_pop_list = []
temp_pdf_location = "./temp_pdf/single_epd.pdf"
temp_json_location = "./temp_json"
found_tables_location = "./json_tables"

#Downloads the pdf and puts it in the temp_pdf directory
def download_epd_pdf(url):
    r = requests.get(url, allow_redirects=True)
    with open(temp_pdf_location, 'wb') as f:
        f.write(r.content)

#Extracts all the tables from one epd link as json and puts them in temp_json folder
def populate_temp_json():
    tables = camelot.read_pdf(temp_pdf_location, flavor='stream', pages='all')
    print(f'Found {tables.__len__()} tables at {temp_pdf_location}')
    tables.export('temp_json/table.json', f='json')

def get_all_tables():
    #List of all the files generated by camelot
    tableList = []
    for root, dirs, files in os.walk(temp_json_location):
        for file in files:
            toAppend = os.path.join("./temp_json",file).replace('\\', '/')
            tableList.append(toAppend)

    return tableList

#Goes through all the created files and finds the exact needed table 
def find_exact_table(tableList):
    for table in tableList:
        jsonFile = json.load(open(table))
        
        #TODO
        #Replace this if with a nicer thing, maybe add a list of all the needed values; this problem will solve itself with improved accuraty (using GhostScraper)
        if 'ENVIRONMENTAL IMPACTS PER KG' in str(jsonFile) or 'RESOURCE USE PER DECLARED UNIT' in str(jsonFile) or 'Miljøpåvirkninger, 15 cm tyk væg' in str(jsonFile):
            print(f'! Found the table at {table} !')
            #TODO, get the exact table location by splitting the table after page - <> and table - <> 
            #will greatly improve accuracy
            #table_location = table.split("word",1)
            return table

#Removes all the contents of the temp_json folder, !use only after the python object has been created!
def flush_temp_json():
    filelist = [ f for f in os.listdir('./temp_json') if f.endswith(".json") ]
    for f in filelist:
        os.remove(os.path.join('./temp_json', f))

#populates a python object from a json file
def populate_epd_object_from_json(table):
    whole_table = json.load(open(table))
    print(whole_table[1])
    print(whole_table[2])
    print(f'Entries Found {len(whole_table[2])}')
    first_row = whole_table[1]
    print(whole_table[1])
    #TODO

def move_found_json_table(table, it):
    if table != None:
        #TODO
        #Make a nicer way of naming the saved tables
        print (f'Saving {table}')
        shutil.move(table, found_tables_location)


def extract(url_list, limit = 1):
    print(f'! --- Found {len(url_list)} epd files --- ! \n>Starting the extraction...')
    iteration = 1
    for url in url_list:

        print(f'Starting Extraction Table: {iteration} at {url}')

        try:

            download_epd_pdf(url)

            populate_temp_json()
            
            all_tables = get_all_tables()

            print(f'All extracted tables: {all_tables}')

            needed_table = find_exact_table(all_tables)
            
            print(f'Needed Table: {needed_table}')

            move_found_json_table(needed_table, iteration)

        except Exception as e:
            print("An exception occurred: ", e) 
            flush_temp_json()

        finally:
            flush_temp_json()

        iteration += 1
        if iteration > limit:
            break

        
# get_json_tables(temp_pdf_location)
# flush_temp_json()
# download_epd_pdf('https://www.epddanmark.dk/media/e2rbzqnl/md-20010-en_adfil.pdf')

# populate_epd_object_from_json('./temp_json/table-page-8-table-1.json')
# find_exact_table(get_all_tables())
