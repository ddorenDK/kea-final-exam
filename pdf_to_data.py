import requests
import camelot
import os
import json
import shutil

from link_to_pdf import link_to_pdf

#TODO
#Extract a links to pdf file

#TODO
#Move this all to a better location
epd_pop_list = []
temp_pdf_location = "./temp_pdf/single_epd.pdf"
temp_json_location = "./temp_json"
found_tables_location = "./json_tables"




def get_list_json_tables(tables_location):
    #List of all the files generated by camelot
    tableList = []
    for root, dirs, files in os.walk(tables_location):
        for file in files:
            toAppend = os.path.join(tables_location,file).replace('\\', '/')
            tableList.append(toAppend)

    return tableList

#Goes through all the created files and finds the exact needed table 
def find_exact_table(tableList):
    for table in tableList:
        jsonFile = json.load(open(table))
        
        #TODO
        #Replace this if with a nicer thing, maybe add a list of all the needed values; this problem will solve itself with improved accuraty (using GhostScraper)
        if 'ENVIRONMENTAL IMPACTS PER KG' in str(jsonFile) or 'RESOURCE USE PER DECLARED UNIT' in str(jsonFile) or 'Miljøpåvirkninger, 15 cm tyk væg' in str(jsonFile):
            print(f'! Found the table at {table} !')
            #TODO, get the exact table location by splitting the table after page - <> and table - <> 
            #will greatly improve accuracy
            #table_location = table.split("word",1)
            return table

#Removes all the contents of the temp_json folder, !use only after the python object has been created!
def flush_temp_json():
    filelist = [ f for f in os.listdir('./temp_json') if f.endswith(".json") ]
    for f in filelist:
        os.remove(os.path.join('./temp_json', f))


#TODO
#Use 'panda' package to work with the material data and put it correctly into the epd object 
#populates a python object from a json file
def populate_epd_object_from_json(table):
    whole_table = json.load(open(table))
    print(whole_table[1])
    print(whole_table[2])
    print(f'Entries Found {len(whole_table[2])}')
    first_row = whole_table[1]
    print(whole_table[1])
    

def move_found_json_table(table, debug = False):
    if debug:
        print
    
        print (f'Saving {table}')
        shutil.move(table, found_tables_location)


def extract(url_list, limit = 1):
    print(f'! --- Found {len(url_list)} epd files --- ! \n>Starting the extraction...')
    iteration = 1
    for url in url_list:

        print(f'Starting Extraction Table: {iteration} at {url}')

        try:

            link_to_pdf(url)

            populate_temp_json()
            
            all_tables = get_all_tables()

            print(f'All extracted tables: {all_tables}')

            needed_table = find_exact_table(all_tables)
            
            print(f'Needed Table: {needed_table}')

            move_found_json_table(needed_table, iteration)

        except Exception as e:
            print("An exception occurred: ", e) 
            flush_temp_json()

        finally:
            flush_temp_json()

        iteration += 1
        if iteration > limit:
            break

        
# get_json_tables(temp_pdf_location)
# flush_temp_json()
# download_epd_pdf('https://www.epddanmark.dk/media/e2rbzqnl/md-20010-en_adfil.pdf')

# populate_epd_object_from_json('./temp_json/table-page-8-table-1.json')
# find_exact_table(get_all_tables())
